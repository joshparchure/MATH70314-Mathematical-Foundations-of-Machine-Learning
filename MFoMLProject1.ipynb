{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC217CFUr34n"
   },
   "source": [
    "# Coursework 1 - Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pIgY-8sr34q"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5r8T5-MWr34r"
   },
   "source": [
    "## Exercise 1: Quick questions [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTApmhnVr34r"
   },
   "source": [
    "#### Question 1:\n",
    "\n",
    "For a given hypothesis $f$, the risk of $f$ is its expected loss taken under the underlying distribution of the data, i.e. $R(f)=\\mathbb{E}_{D}[L(f(\\mathbf{x}),\\mathbf{y})]$. Since the underlying distribution $D$ is unknown, we can approximate the true risk using empirical data. Under the assumption that our data points are i.i.d samples taken from $D$, we define the empirical risk as the average loss over all our data points, i.e. $\\hat{R}(f)=\\frac{1}{N} \\sum_{i=1}^{N} L(f(\\mathbf{x}^{i}),\\mathbf{y}^{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVq2ZKc4r34r"
   },
   "source": [
    "#### Question 2:\n",
    "\n",
    "When training a model $f$ on data $(\\mathbf{x},\\mathbf{y})$, the training error respresents the difference between the model output $f(\\mathbf{x}^i)$ and the (output) data $\\mathbf{y}^{i}$ it was trained using. The generalization error refers to unseen data $(\\mathbf{\\tilde{x}},\\mathbf{\\tilde{y}})$ that the model was not trained on; it represents the difference between the data $\\mathbf{\\tilde{y}}^{i}$ and the predictions $f(\\mathbf{\\tilde{x}}^i)$ generated by the model on the unseen data. We aim to minimise the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIoWqqtEr34r"
   },
   "source": [
    "#### Question 3:\n",
    "\n",
    "In the proof of the inequality we have $e^{-t \\varepsilon} \\mathbb{E}[e^{t(S_{m}-\\mathbb{E}[S_{m}])}] = e^{-t \\varepsilon} \\mathbb{E}[\\prod_{i=1}^{m}e^{t(X_{i}-\\mathbb{E}[X_{i}])}] \\stackrel{(1)}{=} e^{-t \\varepsilon} \\prod_{i=1}^{m} e^{t(X_{i}-\\mathbb{E}[X_{i}])}$ where the step (1) is a consequence of the independence of $X_{i}$: the expectation of a product factorises into a product of expectations under the condition of independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtgHIor1r34s"
   },
   "source": [
    "#### Question 4:\n",
    "\n",
    "- Split the dataset into 3 portions, one for training, one for validation, and one for testing. (An example split would be 80:10:10.)\n",
    "- We use the training dataset to train the first model using regressors from $\\mathcal{H}_1$ and the second model using $\\mathcal{H}_2$ - we use the same data to train both models so that we are able to compare the hypothesis classes without being impacted by the difference in training data used.\n",
    "- We can train multiple models with different hyperparameters on the training set and choose the hyperparameters from the model that performs the best on the validation set (for both hypothesis classes).\n",
    "- Then we can see if our models generalise well to unseen data by testing the best performing model from $\\mathcal{H}_1$ and the best performing model from $\\mathcal{H}_2$ on the same testing set. Again, using the same testing set ensures our comparison of the generalisation error using $\\mathcal{H}_1$ or $\\mathcal{H}_2$ is not dependent on the data used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_2L3rs5r34s"
   },
   "source": [
    "#### Question 5:\n",
    "\n",
    "The generalisation bounds in section 1.4 only apply to a fixed hypothesis or a finite hypotheisis class. Rademacher complexity allows us to understand how large hypothesis classes are even if they are of infinite cardinality. This is  important since the majority of classes we consider are infinite, e.g. linear models: $\\mathcal{H} = \\{ h_w (x) = \\mathbf{w}^T \\mathbf{x} + \\mathbf{b} : ||\\mathbf{w}|| \\leq 1\\}$ is infinite since we can choose infinitely many parameters $\\mathbf{w}$ and $\\mathbf{b}$.\n",
    "\n",
    "Since Rademacher complexity tests how well elements of the hypothesis class can fit to random noise, it gives a measure of how rich the hypothesis class is. The generalisation bounds however simply bound the true risk using the empirical risk and do not consider the richness of the hypothesis class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gzqEo-5r34s"
   },
   "source": [
    "#### Question 6:\n",
    "\n",
    "$\\beta^{k+1} = \\beta^{k} + \\alpha_{k} \\theta_{k}$ where $ \\theta_{k} = -\\Big(\\nabla f(\\beta^k)+\\eta \\theta_{k-1}\\Big)$.\n",
    "\n",
    "We start with looking at the convex setting. Since the update step for $\\beta^{k}$ also considers the previous step, we can intuitively think of the path of $\\beta^k$ as the path traced by a weighted ball rolling around a valley with different contours. If the ball goes down a steep section of the valley, it will gain momentum and its future displacement will have a greater component in that direction. If it then goes does a less steep/flatter region, the ball's momentum will cause it to travel down this steep region faster. So rather than 'zig-zagging' in the successive directions of steepest descent (which could lead to a very indirect path to the minimum), the ball's momentum will cause us to have a faster rate of convergence than regular GD.\n",
    "\n",
    "In a non-convex setting, the ball will have enough momentum to keep travelling over any small mounds (local maxima) or through small ditches (local minima) or in the valley, whereas regular GD with a poorly chosen learning rate could mean we get trapped at a local minimum. As such we might be able to get convergence with momentum where we could not with regular GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejJ9ANwer34s"
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Exercise 2: Statistical learning theory [7 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA14LsF_r34s"
   },
   "source": [
    "\n",
    "### Question 1 [1 point]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5lb4wH3r34s"
   },
   "source": [
    "$\\begin{align*}\n",
    "    f_{S_1,k} &\\in \\underset{f\\in \\mathcal{F}_k}{\\operatorname{argmin}} \\frac{1}{S_1} \\sum_{i=1}^{|S_{1}|} L(f(\\mathbf{x}^{i}),\\mathbf{y}^{i}) \\\\\n",
    "    &= \\frac{1}{(1-\\alpha)N} \\underset{f\\in \\mathcal{F}_k}{\\operatorname{argmin}} \\sum_{i=1}^{(1-\\alpha)N} L(f(\\mathbf{x}^{i}),\\mathbf{y}^{i})\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIaQymTsr34t"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 2 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yKKcW36r34t"
   },
   "source": [
    "The result follows by the application of the union bound.\n",
    "\n",
    "Note that the event in the LHS probability is a subset of a union of events: $\\big\\{ \\sup_{k \\geq 1}|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| > \\varepsilon \\big\\} \\subseteq \\bigcup_{k \\geq 1} \\big\\{ |\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| \\geq \\varepsilon \\big\\}$.\n",
    "\n",
    "For some justifiction of the above statement, define $A := \\big\\{ \\sup_{k \\geq 1}|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| > \\varepsilon \\big\\}$, $B := \\bigcup_{k \\geq 1} \\big\\{ |\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| \\geq \\varepsilon \\big\\}$, $N(k) := |\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})|$ and $C := \\{N(k) : k \\geq 1 \\}$. In $A$ we require the least upper bound of $C$ to exceed $\\varepsilon$. In $B$ we require at least one element of $C$ to be greater than or equal to $\\varepsilon$. If the least upper bound is attained then $A = B$. If the least upper bound is not attained, then any element of $C$ is strictly smaller than the least upper bound. Therefore if the least upper bound is greater than $\\varepsilon$, at least one element of $C$ will be greater than equal to $\\varepsilon$ also (being unable to such an element contradicts the least upper bound property), meaning $A \\subseteq B$.\n",
    "\n",
    "(Aside: A deterministic counterexample showing $B \\not\\subseteq A$ could be $N(k) = 5 - \\frac{1}{k}$. With $\\varepsilon = 4.9$ we have $N(10) = 4.9 \\geq \\varepsilon = 4.9$ but $\\sup_{k \\geq 1} C = 5 \\not> \\varepsilon = 4.9$.)\n",
    "\n",
    "<br>\n",
    "\n",
    "Applying the union bound yields\n",
    "\n",
    "$ \\begin{align*}\n",
    "  \\mathbb{P}\\big(\\sup_{k \\geq 1}|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| > \\varepsilon\\big) &\\leq \\mathbb{P}\\big( \\bigcup_{k \\geq 1} \\big\\{ |\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| \\geq \\varepsilon \\big\\} \\big) \\\\\n",
    "   &\\leq \\sum_{k \\geq 1}\\mathbb{P}\\big(|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| \\geq \\varepsilon\\big)\n",
    "\\end{align*}$\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEdyYGVjr34t"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 3 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cjaENzer34t"
   },
   "source": [
    "Using the law of total expectation/tower property of conditional expectation we have\n",
    "\n",
    "$\\begin{align*}\n",
    "    \\mathbb{P}(A) &= \\mathbb{E}[\\mathbb{1}_{A}] \\\\\n",
    "    &= \\mathbb{E}[\\mathbb{E}[\\mathbb{1}_{A} | S_{1}]] \\\\\n",
    "    &= \\mathbb{E}[\\mathbb{P}(A | S_{1})].\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "We apply this with $A=\\{|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| \\geq \\varepsilon \\}$ yielding\n",
    "\n",
    "$ \\mathbb{P} \\big(|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) \\ | \\geq \\varepsilon \\big) = \\mathbb{E} \\big[\\mathbb{P}\\big(|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) \\ | \\geq \\varepsilon \\ | \\ S_{1}\\big)\\big]$ as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0guVOgyr34u"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 4 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z7Kfgp6r34u"
   },
   "source": [
    "By definition we have the true risk of the ERM as $\\mathcal{R}(f_{S_1,k}) = \\mathbb{E}_{D}[L(f_{S_1,k}(\\mathbf{x}),\\mathbf{y})]$ and the empirical risk as $\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) = \\frac{1}{|S_2|} \\sum_{i=1}^{|S_2|} L(f_{S_1,k}(\\mathbf{x}^i),\\mathbf{y}^i) = \\frac{1}{\\alpha N} \\sum_{i=1}^{\\alpha N} L(f_{S_1,k}(\\mathbf{x}^i),\\mathbf{y}^i)$. Note that in the second expression we take the empirical risk over all samples in $S_2$ of the ERM calculated over samples in $S_1$.\n",
    "\n",
    "We have the probability of an event conditioned on $S_1$ in $\\mathbb{P}(|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| \\geq \\varepsilon | S_{1})$. The key point is that once we condition on $S_1$, the hypothesis $f_{S_1,k}$ (the ERM over $S_1$) becomes deterministic. As such all the stochasticity/randomness in this expression lies in ${S_2}$ and we can apply Corollary 1.15:\n",
    "\n",
    "$\\begin{align*}\n",
    "    \\mathbb{P}_{S_2 \\sim D^{|S_2|}} \\big( |\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| \\geq \\varepsilon | S_{1} \\big) &\\leq 2 \\exp(-2 \\cdot |S_{2}| \\cdot {\\varepsilon}^2) \\\\\n",
    "    &= 2 \\exp(-2 \\alpha N {\\varepsilon}^2).\n",
    "\\end{align*}$\n",
    "\n",
    "Vitally $S_1$ and $S_2$ are sampled from the same distribution $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIbi3AmDr34u"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 5 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMQrmwIer34u"
   },
   "source": [
    "By question 4 (more specifically substituting $\\varepsilon + \\sqrt{\\frac{\\log{k}}{\\alpha N}}$ for $\\varepsilon$ in the result from question 4) we yield\n",
    "\n",
    "$\\begin{align*}\n",
    "    \\mathbb{P}\\big(|\\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k})| \\geq \\varepsilon + \\sqrt{\\frac{\\log{k}}{\\alpha N}} | S_{1}\\big) &\\leq 2 \\exp \\left(-2 \\alpha N \\cdot \\left[ {\\varepsilon + \\sqrt{\\frac{\\log{k}}{\\alpha N}}} \\right]^2 \\right) \\\\\n",
    "    &= 2 \\exp \\left( -2 \\alpha N \\cdot \\left[ {\\varepsilon}^2 + 2 \\varepsilon \\sqrt{\\frac{\\log{k}}{\\alpha N}} + \\frac{\\log{k}}{\\alpha N} \\right] \\  \\right) \\\\\n",
    "    &\\leq 2 \\exp \\left( -2 \\alpha N \\cdot \\left[ {\\varepsilon}^2 + \\frac{\\log{k}}{\\alpha N} \\right] \\right) \\\\\n",
    "    &= 2 \\exp \\left( -2 \\alpha N {\\varepsilon}^2 -2  \\log{k} \\right) \\\\\n",
    "    &= 2 \\exp \\left( -2 \\alpha N {\\varepsilon}^2 \\right) \\cdot \\exp \\left( -2 \\log k \\right) \\\\\n",
    "    &= 2 \\exp \\left( -2 \\alpha N {\\varepsilon}^2 \\right) \\cdot k^{-2} \\\\\n",
    "    &= \\frac{2}{k^2} \\exp \\left( -2 \\alpha N {\\varepsilon}^2 \\right)\n",
    "\\end{align*}$\n",
    "\n",
    "where the 3rd line follows from the fact that $-4 \\alpha N \\varepsilon \\sqrt{\\frac{\\log k}{\\alpha N}} \\leq 0$ and the exponential function is increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1D3rXCyr34u"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 6 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4dgktA8r34u"
   },
   "source": [
    "$\\begin{align}\n",
    "    \\mathbb{P} \\left( \\sup_{k \\geq 1} \\left\\{ | \\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) - \\sqrt{\\frac{\\log k}{\\alpha N}}| \\right\\} > \\varepsilon \\right) &\\leq \\sum_{k=1}^{\\infty} \\mathbb{P} \\left( | \\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) - \\sqrt{\\frac{\\log k}{\\alpha N}}| \\geq \\varepsilon \\right) \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} \\mathbb{E} \\left[ \\mathbb{P} \\left(| \\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) - \\sqrt{\\frac{\\log k}{\\alpha N}}|  \\geq \\varepsilon \\ \\big| \\ S_1 \\right) \\right] \\\\\n",
    "    &\\leq \\sum_{k=1}^{\\infty} \\mathbb{E} \\left[ \\frac{2}{k^2} \\exp{-2 \\alpha N {\\varepsilon}^2} \\right] \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} \\frac{2}{k^2} \\exp(-2 \\alpha N {\\varepsilon}^2) \\\\\n",
    "    &= 2 \\exp(-2 \\alpha N {\\varepsilon}^2) \\cdot \\sum_{k=1}^{\\infty} \\frac{1}{k^2} \\\\\n",
    "    &= 2 \\exp(-2 \\alpha N {\\varepsilon}^2) \\cdot \\frac{\\pi^2}{6} \\\\\n",
    "    &= \\frac{\\pi^2}{3} \\exp(-2 \\alpha N {\\varepsilon}^2) \\\\\n",
    "    &\\leq 4 \\exp(-2 \\alpha N {\\varepsilon}^2)\n",
    "\\end{align} \\\\\n",
    "$\n",
    "\n",
    "where line 1 follows from the result in question 2 and line 2 follows from the law of total expectation. Line 3 follows from question 5 and the fact that the expectation, as an integral, preserves inequalities. The final line follows since $\\pi^2 \\leq 12 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLYwHVwer34u"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 7 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03_aU00jr34v"
   },
   "source": [
    "For any given choice of $k$ we have\n",
    "\n",
    "$$\\mathbb{P} \\left(| \\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) - \\sqrt{\\frac{\\log k}{\\alpha N}}| \\geq \\varepsilon \\right) \\leq \\mathbb{P} \\left( \\sup_{k \\geq 1} \\left\\{ | \\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) - \\sqrt{\\frac{\\log k}{\\alpha N}}| \\right\\} > \\varepsilon \\right)$$\n",
    "\n",
    "by definition of the supremum. We also can apply result from question 6, which means $\\forall \\ \\varepsilon > 0$ we have\n",
    "\n",
    "$\\begin{align}\n",
    "    \\mathbb{P} \\left(| \\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) - \\sqrt{\\frac{\\log k}{\\alpha N}}| \\geq \\varepsilon \\right) &\\leq \\mathbb{P} \\left( \\sup_{k \\geq 1} \\left\\{ | \\mathcal{R}(f_{S_1,k})-\\mathcal{\\hat{R}}_{S_2}(f_{S_1,k}) - \\sqrt{\\frac{\\log k}{\\alpha N}}| \\right\\} > \\varepsilon \\right) \\\\\n",
    "    &\\leq 4 e^{-2 \\alpha N {\\varepsilon}^2}\n",
    "\\end{align}$\n",
    "\n",
    "<br>\n",
    "\n",
    "The above holds for all choices of k; with k as $\\tilde{k}$ we have $f_{S_1,k} = \\tilde{f_{S_2}}$ by definition. The above also holds for all $\\varepsilon > 0$. As such define $\\delta$ by $\\delta = 4 e^{-2 \\alpha N {\\varepsilon}^2} \\iff \\varepsilon = \\sqrt{\\frac{\\log \\frac{4}{\\delta}}{2 \\alpha N}}$.  Since $\\varepsilon \\in (0,\\infty)$ we know $\\delta \\in (0,1)$ since $2 \\alpha N {\\varepsilon}^2 \\in (0,\\infty)$ and the exponential function is strictly bounded by 0 and 1 for $x \\in (0,\\infty)$. Therefore, for all $\\delta \\in (0,1)$ we have\n",
    "\n",
    "$\\mathbb{P} \\left(| \\mathcal{R}(\\tilde{f_{S_2}})-\\mathcal{\\hat{R}}_{S_2}(\\tilde{f_{S_2}}) - \\sqrt{\\frac{\\log \\tilde{k}}{\\alpha N}}| \\geq \\sqrt{\\frac{\\log \\frac{4}{\\delta}}{2 \\alpha N}} \\right) \\leq \\delta$.\n",
    "\n",
    "<br>\n",
    "\n",
    "We have an event which occurs with probability at most $\\delta$. Therefore the complement of the event occurs with probability at least $1-\\delta$:\n",
    "\n",
    "$\\mathbb{P} \\left(| \\mathcal{R}(\\tilde{f_{S_2}})-\\mathcal{\\hat{R}}_{S_2}(\\tilde{f_{S_2}}) - \\sqrt{\\frac{\\log \\tilde{k}}{\\alpha N}}| < \\sqrt{\\frac{\\log \\frac{4}{\\delta}}{2 \\alpha N}} \\right) \\geq 1- \\delta$.\n",
    "\n",
    "We can relax the bound to a $\\leq$ which increases the probability (since we create a superset) so we have\n",
    "\n",
    "$\\mathbb{P} \\left(| \\mathcal{R}(\\tilde{f_{S_2}})-\\mathcal{\\hat{R}}_{S_2}(\\tilde{f_{S_2}}) - \\sqrt{\\frac{\\log \\tilde{k}}{\\alpha N}}| \\leq \\sqrt{\\frac{\\log \\frac{4}{\\delta}}{2 \\alpha N}} \\right) \\geq 1- \\delta$.\n",
    "\n",
    "So, with probability at least $1-\\delta$ we have $| \\mathcal{R}(\\tilde{f_{S_2}})-\\mathcal{\\hat{R}}_{S_2}(\\tilde{f_{S_2}}) - \\sqrt{\\frac{\\log \\tilde{k}}{\\alpha N}}| \\leq \\sqrt{\\frac{\\log \\frac{4}{\\delta}}{2 \\alpha N}}$. In removing the absolute value we get two inequalities, one of which reads $\\mathcal{R}(\\tilde{f_{S_2}})-\\mathcal{\\hat{R}}_{S_2}(\\tilde{f_{S_2}}) - \\sqrt{\\frac{\\log \\tilde{k}}{\\alpha N}} \\leq \\sqrt{\\frac{\\log \\frac{4}{\\delta}}{2 \\alpha N}} \\iff \\mathcal{R}(\\tilde{f_{S_2}})-\\mathcal{\\hat{R}}_{S_2}(\\tilde{f_{S_2}}) \\leq \\sqrt{\\frac{\\log \\tilde{k}}{\\alpha N}} + \\sqrt{\\frac{\\log \\frac{4}{\\delta}}{2 \\alpha N}}$ as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLi4WKtYr34v"
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Exercise 3: Optimization questions [4.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXNjTIAcr34v"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 1 [0.5 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dLbeaqer34v"
   },
   "source": [
    "Take $\\lambda \\in [0,1]$ and $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^n$.\n",
    "\n",
    "$\\begin{align}\n",
    "    f(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) &= || \\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y} || \\\\\n",
    "    &\\leq || \\lambda \\mathbf{x} || + ||(1-\\lambda) \\mathbf{y}|| \\\\\n",
    "    &=  \\lambda || \\mathbf{x} || + (1-\\lambda)|| \\mathbf{y}|| \\\\\n",
    "    &=  \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y})\n",
    "\\end{align}$\n",
    "\n",
    "where we used the triangle inequality in line 2 and homogeneity of norms in line 3. Therefore $f$ is convex by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRwxbgcRr34v"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 2 [0.5 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_eghUVLr34v"
   },
   "source": [
    "Take $\\lambda \\in [0,1]$ and $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^d$.\n",
    "\n",
    "$\\begin{align}\n",
    "    g(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) &= f(A(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) +\\mathbf{b}) \\\\\n",
    "    &=  f(\\lambda A \\mathbf{x} + (1-\\lambda) A \\mathbf{y} +\\mathbf{b}) \\\\\n",
    "    &=  f(\\lambda A \\mathbf{x} + (1-\\lambda) A \\mathbf{y} +\\lambda \\mathbf{b} + (1-\\lambda) \\mathbf{b}) \\\\\n",
    "    &=  f(\\lambda (A \\mathbf{x} + \\mathbf{b}) + (1-\\lambda) (A \\mathbf{y} + \\mathbf{b})) \\\\\n",
    "    &\\leq \\lambda  f(A \\mathbf{x} + \\mathbf{b}) + (1-\\lambda) f(A \\mathbf{y} + \\mathbf{b}) \\\\\n",
    "    &= \\lambda g(\\mathbf{x}) + (1- \\lambda ) g(\\mathbf{y})\n",
    "\\end{align}$\n",
    "\n",
    "where we used the convexity of $f$ in the 5th line. Therefore $g$ is convex by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u98veWWOr34w"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 3 [0.5 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-lJXgt8r34w"
   },
   "source": [
    "Take $\\lambda \\in [0,1]$ and $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^n$.\n",
    "\n",
    "$\\begin{align}\n",
    "    h(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) &= f_{1}(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) + f_{2}(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) \\\\\n",
    "    &\\leq \\lambda f_{1}(\\mathbf{x}) + (1-\\lambda) f_{1} (\\mathbf{y}) + \\lambda f_{2}(\\mathbf{x}) + (1-\\lambda) f_{2} (\\mathbf{y}) \\\\\n",
    "    &= \\lambda (f_{1}(\\mathbf{x}) + f_{1}(\\mathbf{y})) + (1-\\lambda) (f_{2}(\\mathbf{x}) + f_{2}(\\mathbf{y}))\\\\\n",
    "    &= \\lambda h(\\mathbf{x}) + (1-\\lambda)h(\\mathbf{y})\n",
    "\\end{align}$\n",
    "\n",
    "where we used the convexity of $f_1$ and $f_2$ in the first line. This is the definition of convexity so $h$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns_hHZRmr34w"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 4 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzKsc4uHr34w"
   },
   "source": [
    "If a function is twice differentiable then convexity is equivalent to the Hessian being positive semidefinite (equation (2.11) in the notes).\n",
    "\n",
    "Proof: ($\\impliedby$ direction for the purposes using this as a sufficient condition for convexity)\n",
    "\n",
    "If we consider f as a scalar function of the variable t, we have $\\frac{d}{dt}[f(t \\mathbf{x} + (1-t) \\mathbf{y})] = \\nabla f(t \\mathbf{x} + (1-t)\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y})$ and $\\frac{d^2}{dt^2}[f(t \\mathbf{x} + (1-t) \\mathbf{y})] = (\\mathbf{x}-\\mathbf{y})^T \\nabla^2 f(t \\mathbf{x} + (1-t)\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y})$.\n",
    "\n",
    "We can therefore apply Taylor's theorem with remainder to $f$ at points $t=1$ and $t=0$ yielding\n",
    "\n",
    "$f(\\mathbf{x}) = f(\\mathbf{y}) + \\nabla f(\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y}) + \\int_0^1 (1-t) (\\mathbf{x}-\\mathbf{y})^T \\nabla^2 f(t \\mathbf{x} + (1-t)\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y}) \\ dt$.\n",
    "\n",
    "Under the premise of the Hessian being positive semidefinite, we know that\n",
    "$\\int_0^1 (1-t) (\\mathbf{x}-\\mathbf{y})^T \\nabla^2 f(t \\mathbf{x} + (1-t)\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y}) \\ dt \\geq \\int_0^1 (1-t) \\ dt = \\frac{1}{2} \\geq 0$.\n",
    "\n",
    "Therefore $f(\\mathbf{x}) \\geq f(\\mathbf{y}) + \\nabla f(\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y})$ and $f$ is convex as required.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\nabla f(\\mathbf{x}) &= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{d}{dx} \\big(\\ln(1+x) \\, \\big) \\ \\Big|_{x=\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)}  \\nabla(\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle) \\\\\n",
    "    &= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)} \\cdot -y_i \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle) \\, a_i\\\\\n",
    "    &= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{-y_i \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)}{1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)} a_i\n",
    "  \\end{align}$$\n",
    "  \n",
    "where we used the chain rule in the first line.\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\nabla^2 f(\\mathbf{x}) &= \\nabla \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{-y_i \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)}{1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)} a_i \\right) \\\\\n",
    "    &= \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\left(  \\frac{-y_i \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)}{1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)} a_i \\right) \\\\\n",
    "    &= \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\left(  \\frac{-y_i \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)}{1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)} \\right) a_i^T \\\\\n",
    "    &= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{[1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle) \\cdot -y_i \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle) \\cdot -y_i a_i] - [-y_i \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle) \\cdot \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle) \\cdot -y_i a_i]}{(1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle))^2} a_i^T \\\\\n",
    "    &= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{{y_i}^2 (1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)) (\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle)) a_i - y_{i}^2 (\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle))^2 a_i}{(1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle))^2} a_i^T \\\\\n",
    "    &= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{y_{i}^2 \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle a_i a_i^T}{(1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle))^2} \\\\\n",
    "    &\\succeq 0\n",
    "  \\end{align}$$\n",
    "  \n",
    "where we used the quotient rule in the fourth line. The final line follows since $y_{i}^2 , \\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle) \\geq 0$ so $\\frac{y_{i}^2 \\exp(-y_{i} \\langle \\mathbf{x}, a_{i} \\rangle)}{(1+\\exp(-y_{i} \\langle \\mathbf{x},a_{i} \\rangle))^2} \\geq 0$ and $a_{i} a_{i}^T$ is positive semidefinite as an outer product: $z^T a_i a_i^T z = (z^T a_i) (z^T a_i)^T = (z^T a_i)^2 \\geq 0$ ($z^T a_i$ is a scalar and so $z^T a_i = (z^T a_i)^T \\in \\mathbb{R}$).\n",
    "\n",
    "Therefore $f$ is convex since its Hessian is positive semidefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1IRK40ar34w"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 5 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "renRF2RBr34w"
   },
   "source": [
    "If $f$ is twice-differentiable, $f$ is $\\mu$-strongly convex iff $\\mathbf{z}^T \\nabla^2 f(\\mathbf{x}) \\mathbf{z} \\geq \\mu ||\\mathbf{z}||_2 ^2$ $\\forall \\, \\mathbf{z} \\in \\mathbb{R}^d$ and $\\mathbf{x} \\in \\mathbb{R}^d$ (proposition (2.13) in the notes).\n",
    "\n",
    "Proof: ($\\impliedby$ direction for the purposes using this as a sufficient condition for $\\mu$-strong convexity)\n",
    "\n",
    "In the exact same way as the convexity proof in question 4 we can apply Taylor's theorem with integral form remainder yielding\n",
    "\n",
    "$$\\begin{align}\n",
    "    f(\\mathbf{x}) &= f(\\mathbf{y}) + \\nabla f(\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y}) + \\int_0^1 (1-t) (\\mathbf{x}-\\mathbf{y})^T \\nabla^2 f(t \\mathbf{x} + (1-t)\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y}) \\ dt \\\\\n",
    "    &\\geq f(\\mathbf{y}) + \\nabla f(\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y}) + \\mu ||\\mathbf{x}-\\mathbf{y}||^2 \\cdot \\int_0^1 1-t \\ dt \\\\\n",
    "    &= f(\\mathbf{y}) + \\nabla f(\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y}) + \\mu ||\\mathbf{x}-\\mathbf{y}||^2 \\cdot \\frac{1}{2} \\\\\n",
    "    &= f(\\mathbf{y}) + \\nabla f(\\mathbf{y})^T (\\mathbf{x}-\\mathbf{y}) + \\frac{\\mu}{2} ||\\mathbf{x}-\\mathbf{y}||^2\n",
    "\\end{align}$$\n",
    "\n",
    "which is the definition of $\\mu$-strong convexity as required. The second line follows from the premise $\\mathbf{z}^T \\nabla^2 f(\\mathbf{x}) \\mathbf{z} \\geq \\mu ||\\mathbf{z}||^2$.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\nabla f(\\mathbf{x}) &= \\frac{1}{2} \\nabla(||A \\mathbf{x} -\\mathbf{b}||_2 ^2) \\\\\n",
    "    &= \\frac{1}{2} \\nabla \\big((A \\mathbf{x} -\\mathbf{b})^T(A \\mathbf{x} -\\mathbf{b}) \\big) \\\\\n",
    "    &= \\frac{1}{2} \\nabla \\big((\\mathbf{x}^T A^T - \\mathbf{b}^T) (A \\mathbf{x} -\\mathbf{b}) \\big) \\\\\n",
    "    &= \\frac{1}{2} \\nabla(\\mathbf{x}^T A^T A \\mathbf{x} - \\mathbf{x}^T A^T \\mathbf{b} - \\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b}) \\\\\n",
    "    &= \\frac{1}{2} \\nabla(\\mathbf{x}^T A^T A \\mathbf{x} - 2 \\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b}) \\\\\n",
    "    &= \\frac{1}{2} (2 A^T A \\mathbf{x} - 2 A^T \\mathbf{b}) \\\\\n",
    "    &= A^T(A \\mathbf{x} - \\mathbf{b})\n",
    "  \\end{align}$$\n",
    "  \n",
    "Then\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\nabla^2 f(\\mathbf{x}) &= \\nabla \\big(A^T(A \\mathbf{x} - \\mathbf{b}) \\big) \\\\\n",
    "    &= \\nabla (A^T A \\mathbf{x} - A^T \\mathbf{b}) \\\\\n",
    "    &= A^T A.\n",
    "  \\end{align}$$\n",
    "  \n",
    "<br>\n",
    "  \n",
    "Since $A$ is of full (column) rank we know that $A$ is positive definite:\n",
    "\n",
    "- $\\mathbf{x}^T A^T A \\mathbf{x} = (A\\mathbf{x})^T (A\\mathbf{x}) = ||A\\mathbf{x}||_2 ^2 \\geq 0 \\, \\forall \\, \\mathbf{x} \\in \\mathbb{R}^d$. So  $A^T A$ is positive semi-definite.\n",
    "- $A$ having full column rank means the columns of $A$ are linearly independent; denote these columns as $(\\mathbf{a}_{i})_{i=1}^{d}$.\n",
    "- This is equivalent to the equation $\\sum_{i=1}^{d} x_i \\mathbf{a}_i = 0$ having sole solution $x_{i}=0 \\, \\forall \\, i \\in [d]$, i.e. $\\mathbf{x}=\\mathbf{0}$.\n",
    "- Therefore, $\\mathbf{x}^T A^T A \\mathbf{x} = 0 \\iff \\mathbf{x}= \\mathbf{0}$.\n",
    "- Since we know $\\mathbf{x}^T A^T A \\mathbf{x} \\geq 0$, altogether we know that $\\mathbf{x}^T A^T A \\mathbf{x} > 0$ for all non-zero  $\\mathbf{x}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Since $A^T A$ is symmetric and positive definite, we know that all eigenvalues of $A^T A$ are strictly positive. Denote the smallest eigenvalue as $\\lambda_{\\text{min}}(A^T A)$.\n",
    "\n",
    "By the spectral theorem, we express $A^T A = Q \\Lambda Q^T$ where $Q$ is an orthogonal matrix of eigenvectors of $A^T A$ and $\\Lambda = \\text{diag}(\\lambda_{1},...,\\lambda_{d})$ where the $\\lambda_{i}$ are eigenvalues of $A^T A$.\n",
    "\n",
    "Then $\\mathbf{z}^T \\nabla^2 f(\\mathbf{x}) \\mathbf{z} = \\mathbf{z}^T Q \\Lambda Q^T \\mathbf{z} = (Q^T \\mathbf{z})^T \\Lambda (Q^T \\mathbf{z})$. Since $Q$ is an orthogonal matrix, we have $y = Q^T \\mathbf{z}$ is a rotation of $\\mathbf{z}$.\n",
    "\n",
    "Then $\\mathbf{y}^T \\Lambda \\mathbf{y} = \\sum_{i=1}^{d} \\lambda_{i} y_{i}^2 \\geq \\sum_{i=1}^{d} \\lambda_{\\text{min}}(A^T A) y_{i}^2 = \\lambda_{\\text{min}}(A^T A) ||\\mathbf{y}||_2 ^2$.\n",
    "\n",
    "$||\\mathbf{y}||_2 = ||\\mathbf{z}||_2$ since  $y = Q^T \\mathbf{z}$ and $Q$ is orthogonal.\n",
    "\n",
    "Therefore we have  $\\mathbf{z}^T \\nabla^2 f(\\mathbf{x}) \\mathbf{z} \\geq \\lambda_{\\text{min}}(A^T A) ||\\mathbf{z}||_2 ^2$ $\\forall \\, \\mathbf{z} \\in \\mathbb{R}^d$ and $\\mathbf{x} \\in \\mathbb{R}^d$ and hence $f$ is $\\lambda_{\\text{min}}(A^T A)$-strongly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DHoerRor34x"
   },
   "source": [
    "***\n",
    "\n",
    "### Question 6 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dOHOGhir34x"
   },
   "source": [
    "$f(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\langle \\nabla f(\\mathbf{x}), \\mathbf{y}- \\mathbf{x} \\rangle + \\frac{\\mu}{2} ||\\mathbf{y}-\\mathbf{x} ||_2 ^2$ holds for all $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^d$.\n",
    "\n",
    "Multiplying the inequality by $-1$ yields\n",
    "\n",
    "$-f(\\mathbf{y}) \\leq -f(\\mathbf{x}) - \\langle \\nabla f(\\mathbf{x}), \\mathbf{y}- \\mathbf{x} \\rangle - \\frac{\\mu}{2} ||\\mathbf{y}-\\mathbf{x} ||_2 ^2$ for all $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^d$.\n",
    "\n",
    "In particular, taking $\\mathbf{y} = \\mathbf{x}^{*}$ yields\n",
    "\n",
    "$-f(\\mathbf{x}^{*}) \\leq -f(\\mathbf{x}) - \\langle \\nabla f(\\mathbf{x}), \\mathbf{x}^{*}- \\mathbf{x} \\rangle - \\frac{\\mu}{2} ||\\mathbf{x}^{*}-\\mathbf{x} ||_2 ^2$ for all $\\mathbf{x} \\in \\mathbb{R}^d$\n",
    "\n",
    "$f(\\mathbf{x}) -f(\\mathbf{x}^{*}) \\leq - \\langle \\nabla f(\\mathbf{x}), \\mathbf{x}^{*}- \\mathbf{x} \\rangle - \\frac{\\mu}{2} ||\\mathbf{x}^{*}-\\mathbf{x} ||_2 ^2$ for all $\\mathbf{x} \\in \\mathbb{R}^d$ (*)\n",
    "\n",
    "<br>\n",
    "\n",
    "In order to generate a term like $- \\frac{1}{2 \\mu}||\\nabla f(\\mathbf{x})||_2 ^2$ using the equality $||\\mathbf{a}+\\mathbf{b}||_2 ^2 = ||\\mathbf{a}||_2 ^2 + 2 \\langle \\mathbf{a},\\mathbf{b} \\rangle + ||\\mathbf{b}||_2 ^2$ we can take $\\mathbf{a} = \\frac{1}{\\sqrt{\\mu}} \\nabla f(\\mathbf{x})$. To keep the term $\\langle \\nabla f(\\mathbf{x}), \\mathbf{x}^{*}- \\mathbf{x} \\rangle$ we compensate by taking $\\mathbf{b} = \\sqrt{\\mu} (\\mathbf{x}-\\mathbf{x}^*)$. This yields\n",
    "\n",
    "$$\\begin{align}\n",
    "    ||\\frac{1}{\\sqrt{\\mu}} \\nabla f(\\mathbf{x}) + \\sqrt{\\mu}(\\mathbf{x}^* - \\mathbf{x})||_2 ^2 &= ||\\frac{1}{\\sqrt{\\mu}} \\nabla f(\\mathbf{x})||_2 ^2 + ||\\sqrt{\\mu} (\\mathbf{x}^* - \\mathbf{x})||_2 ^2 + 2 \\langle \\frac{1}{\\sqrt{\\mu}} \\nabla f(\\mathbf{x}) , \\sqrt{\\mu} (\\mathbf{x}-\\mathbf{x}^*)\\rangle \\\\\n",
    "    &= \\frac{1}{\\mu} || \\nabla f(\\mathbf{x})||_2 ^2 + \\mu || (\\mathbf{x}^* - \\mathbf{x})||_2 ^2 + 2 \\langle \\nabla f(\\mathbf{x}) , \\mathbf{x}-\\mathbf{x}^* \\rangle\n",
    "  \\end{align}$$\n",
    "  \n",
    "<br>\n",
    "  \n",
    "As such $\\langle \\nabla f(\\mathbf{x}) , \\mathbf{x}-\\mathbf{x}^* \\rangle = \\frac{1}{2}||\\frac{1}{\\sqrt{\\mu}} \\nabla f(\\mathbf{x}) + \\sqrt{\\mu}(\\mathbf{x}^* - \\mathbf{x})||_2 ^2  - \\frac{1}{2 \\mu} || \\nabla f(\\mathbf{x})||_2 ^2 - \\frac{\\mu}{2} ||(\\mathbf{x}^* - \\mathbf{x})||_2 ^2$ which we can substitute into (*) yielding\n",
    "\n",
    "$$\\begin{align}\n",
    "    f(\\mathbf{x}) -f(\\mathbf{x}^{*}) &\\leq - \\frac{1}{2}||\\frac{1}{\\sqrt{\\mu}} \\nabla f(\\mathbf{x}) + \\sqrt{\\mu}(\\mathbf{x}^* - \\mathbf{x})||_2 ^2  + \\frac{1}{2 \\mu} || \\nabla f(\\mathbf{x})||_2 ^2 + \\frac{\\mu}{2} ||(\\mathbf{x}^* - \\mathbf{x})||_2 ^2 - \\frac{\\mu}{2} ||\\mathbf{x}^{*}-\\mathbf{x} ||_2 ^2 \\\\\n",
    "    &= - \\frac{1}{2}||\\frac{1}{\\sqrt{\\mu}} \\nabla f(\\mathbf{x}) + \\sqrt{\\mu}(\\mathbf{x}^* - \\mathbf{x})||_2 ^2  + \\frac{1}{2 \\mu} || \\nabla f(\\mathbf{x})||_2 ^2 \\\\\n",
    "    &\\leq \\frac{1}{2 \\mu} || \\nabla f(\\mathbf{x})||_2 ^2\n",
    "  \\end{align}$$\n",
    "  \n",
    "  as required since $- \\frac{1}{2}||\\frac{1}{\\sqrt{\\mu}} \\nabla f(\\mathbf{x}) + \\sqrt{\\mu}(\\mathbf{x}^* - \\mathbf{x})||_2 ^2 \\leq 0$ always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AVGeoSGr34x"
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Exercise 2: Gradient descent and linear regression [5.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Zr45jSHqr34x"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "X = torch.tensor(diabetes.data)\n",
    "Y = torch.tensor(diabetes.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoQmFCvCCeZp",
    "outputId": "492f4f98-16e6-439e-824d-3f599f86e335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10)\n",
      "torch.Size([442, 10])\n"
     ]
    }
   ],
   "source": [
    "print(torch.linalg.matrix_rank(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmiTaWGkr34z"
   },
   "source": [
    "***\n",
    "### Question 1 [0.5 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCRIRNnPoaVq"
   },
   "source": [
    "We re-define $X$ as\n",
    "\n",
    "$X = \\begin{pmatrix}\n",
    "1& x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(10)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots &\\vdots \\\\\n",
    "1& x_N^{(1)} & x_N^{(2)} & \\cdots & x_N^{(10)}\n",
    "\\end{pmatrix}$ and $ \\boldsymbol{\\theta} = \\begin{pmatrix}\n",
    "c \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_{10} \\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "such that $X \\boldsymbol{\\theta} = X \\boldsymbol{\\beta} + c$. Now we can learn all parameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fGbSoCvUr34z"
   },
   "outputs": [],
   "source": [
    "# Extract the number of rows of the data\n",
    "X_shape = X.shape\n",
    "N = X_shape[0]\n",
    "\n",
    "# Add a column of 1s at the beginning of X\n",
    "X_augmented = torch.cat((torch.ones(N,1), X), dim=1)\n",
    "X_augmented_shape = X_augmented.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlYNBPwxr34z"
   },
   "source": [
    "***\n",
    "### Question 2 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CxbDSl8Mr34z"
   },
   "outputs": [],
   "source": [
    "# Here we set a random seed so we can reproduce our results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Randomly choose a permutation\n",
    "perm = torch.randperm(X_augmented.shape[0])\n",
    "\n",
    "# Split the dataset for the given split ratio (80:20) using the permutation\n",
    "train_size = 353\n",
    "train_idx = perm[:train_size]\n",
    "test_idx = perm[train_size:]\n",
    "\n",
    "# Define the split dataset with the determined indices above\n",
    "X_train = X_augmented[train_idx]\n",
    "y_train = Y[train_idx]\n",
    "X_test = X_augmented[test_idx]\n",
    "y_test = Y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqJdFOgvr34z"
   },
   "source": [
    "***\n",
    "### Question 3 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5VthQ5vr34z",
    "outputId": "168e8e3d-787b-4345-d3a5-4ee79dc4225e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares testing Error:  2751.3988279994687\n"
     ]
    }
   ],
   "source": [
    "# Here we define a function which returns the explicit least squares solution given an input X (augmented data matrix) and vector y (targets)\n",
    "def least_squares_solution(X, y):\n",
    "    return torch.inverse(X.T @ X) @ X.T @ y\n",
    "\n",
    "# Find the least squares estimator on the training dataset\n",
    "theta_ls = least_squares_solution(X_train, y_train)\n",
    "\n",
    "# Define a function to compute the MSE between the model output (determined by the matrix X and parameters theta) and the actual targets (y)\n",
    "def compute_mse(X, y, theta):\n",
    "    y_pred = X @ theta\n",
    "    return torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "# Find the MSE for the least squares problem on the testing data\n",
    "ls_test_error = compute_mse(X_test, y_test, theta_ls)\n",
    "\n",
    "print(\"Least squares testing Error: \", ls_test_error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hglxArdGr340"
   },
   "source": [
    "***\n",
    "### Question 4 [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "BJQTfXKUWhO4",
    "outputId": "c77b7611-e0bb-4546-dc6f-21afb9b8a00f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Descent - Testing Error:  2749.125305005122\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.1, epochs=10000):\n",
    "\n",
    "    # Initialise all parameters as 0 (with the same dtype as entries of X)\n",
    "    theta = torch.zeros(X.shape[1], dtype=torch.float64)\n",
    "\n",
    "    n = X.shape[0]\n",
    "    errors = []\n",
    "\n",
    "    # initial_loss = compute_mse(X, y, theta)\n",
    "    # print(\"Initial loss (epoch 0):\", initial_loss.item())\n",
    "\n",
    "    # theta_ols = least_squares_solution(X, y)\n",
    "    # ols_loss = compute_mse(X, y, theta_ols)\n",
    "    # print(\"OLS estimator loss:\", ols_loss.item())\n",
    "\n",
    "    # Iterate the gradient descent algorithm through the given number of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Compute the model predictions\n",
    "        y_pred = X @ theta\n",
    "\n",
    "        # Compute gradients without autograd\n",
    "        # If L = 1/N ||X*theta - Y||^2 then grad L = 2/N X^T (X*theta - Y)\n",
    "        gradients = 2/n * X.T @ (y_pred - y)\n",
    "\n",
    "        # Update parameters in the direction of steepest descent\n",
    "        theta = theta - learning_rate * gradients\n",
    "\n",
    "        # Store the error for each epoch\n",
    "        error = compute_mse(X, y, theta)\n",
    "        errors.append(error.item())\n",
    "\n",
    "    # Return the final parameters theta (we are optimising in the form argmin L(theta))\n",
    "    # Also return the errors for plotting\n",
    "    return theta, errors\n",
    "\n",
    "# Run the gradient descent algorithm\n",
    "theta_gd, train_errors = gradient_descent(X_train, y_train)\n",
    "\n",
    "# Compute the testing errors for gradient descent\n",
    "gd_test_error = compute_mse(X_test, y_test, theta_gd)\n",
    "\n",
    "# Plot a graph of the training error vs epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(0,10000,1), train_errors)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Error (MSE)')\n",
    "plt.title('Training Error vs Epochs in Gradient Descent')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(\"Gradient Descent - Testing Error: \", gd_test_error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVDKX2eeuU6S"
   },
   "source": [
    "The testing error for gradient descent is similar to the testing error for the analytic least squares solution. Since the loss function is convex, gradient descent is guaranteed to converge (to the analytic OLS solution) eventually. The number of epochs is sufficiently large and the learning rate is not too small, and as such the two values are similar. The graph shows that the training error decreases rapidly and then begins to plateau, indicating convergence.\n",
    "\n",
    "<br>\n",
    "\n",
    "I attempted to quantify this using Proposition 2.16 on the training set and claim that this generalises to the MSE and the test data, but was unable to make it work:\n",
    "\n",
    "- First note when $\\lambda = 0$, we have $\\nabla L(\\mathbf{x}) = 2A^T (A\\mathbf{x}-\\mathbf{b})$ and $\\nabla^2 L(x) = 2A^TA$.\n",
    "\n",
    "- By question 5 in the optimisation questions (section 3), when looking at the training data we have $L(x)$ is $\\mu$-strongly convex where $\\mu = \\frac{2}{N_{\\text{train}}} \\lambda_{min} (X_{\\text{train}}^T X_{\\text{train}})$.\n",
    "\n",
    "- We also know that $L(x)$ is $L$-smooth if all the eigenvalues of $\\nabla^2 L(x)$ are less than $L$, and as such $L(x)$ is $\\frac{2}{N_{\\text{train}}} \\lambda_{max} (X_{\\text{train}}^T X_{\\text{train}})$-smooth.\n",
    "\n",
    "- On the training dataset these values are $L = 2.000$ and $\\mu = 3.9223 \\times 10^{-05}$. If a function is $L$-smooth it is also $L'$-smooth for all $L' \\geq L$, and as such we have $L(x)$ is $10$-smooth ($10>2$; we get the desired value of $\\alpha=\\frac{1}{10}=0.1$ for the learning rate).\n",
    "\n",
    "- By Proposition 2.16 we have that\n",
    "\n",
    "- $\\begin{align}\n",
    "L(\\mathbf{x}_{10000}) - L(\\mathbf{x}_\\text{OLS}) &\\leq \\exp(- \\frac{\\mu}{L'} \\cdot 10000) \\cdot (L(\\mathbf{x}_0) - L(\\mathbf{x}_\\text{OLS})) \\\\\n",
    "&\\leq \\exp(- \\frac{3.9223 \\times 10^{-05}}{10} \\cdot 10000) \\cdot (28903 - 2897) \\\\\n",
    "&= 25005\n",
    "\\end{align}$\n",
    "\n",
    "- However this number is much larger than expected. I was expecting this to be smaller, and claim that this inequality will loosely generalise to the testing data and with the MSE (perhaps using equivalence of norms).\n",
    "\n",
    "Below is some of the code I used to find these eigenvalues and values of $L(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNvlWRKxW4z3",
    "outputId": "8f576012-cb59-4a66-ebd0-92b8f1db3801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0000, dtype=torch.float64)\n",
      "tensor(3.9223e-05, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def largest_eigenvalue(A):\n",
    "  eigenvalues, _ = torch.linalg.eig(A)\n",
    "  return torch.max(torch.abs(eigenvalues))\n",
    "\n",
    "def smallest_eigenvalue(A):\n",
    "  eigenvalues, _ = torch.linalg.eig(A)\n",
    "  return torch.min(torch.abs(eigenvalues))\n",
    "\n",
    "print(largest_eigenvalue(2/353*X_train.T @ X_train))\n",
    "print(smallest_eigenvalue(2/353*X_train.T @ X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbxci4ElLwRy",
    "outputId": "a032f384-d8c1-4839-dc0c-07d4e9c9f587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28903.1416, dtype=torch.float64)\n",
      "tensor(2897.8569, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "L_x0 = torch.mean(y_train**2)\n",
    "print(L_x0)\n",
    "\n",
    "y_pred_ols = X_train @ theta_ls\n",
    "L_xols = compute_mse(X_train, y_train, theta_ls)\n",
    "\n",
    "print(L_xols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAAjdLOOr340"
   },
   "source": [
    "***\n",
    "### Question 5 [0.5 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVpRY5FTr340"
   },
   "source": [
    "$\\begin{align}\n",
    "    \\nabla L(x) &= \\nabla(||A \\mathbf{x} - \\mathbf{b}||_2 ^2) +  \\nabla(\\lambda ||\\mathbf{x}||_2 ^2) \\\\\n",
    "    &= 2 A^T (A \\mathbf{x} - \\mathbf{b}) + 2 \\lambda \\mathbf{x}\n",
    "\\end{align}$\n",
    "\n",
    "where the first term is the same as in Exercise 3 Q5 and for the second term we have $\\partial_{i} (||\\mathbf{x}||_2 ^2) = \\partial_{i}(x_1^2 + + ... + x_n^2) = 2x_i$.\n",
    "\n",
    "Solving $\\nabla L(x) = 0$ yields\n",
    "\n",
    "$\\begin{align}\n",
    "    2 A^T (A \\mathbf{x} - \\mathbf{b}) + 2 \\lambda \\mathbf{x} = 0 &\\iff A^T (A \\mathbf{x} - \\mathbf{b}) + \\lambda \\mathbf{x} = 0 \\\\\n",
    "    &\\iff A^T A \\mathbf{x} - A^T \\mathbf{b} + \\lambda \\mathbf{x} = 0 \\\\\n",
    "    &\\iff (A^T A + \\lambda I) \\mathbf{x} = A^T \\mathbf{b}.\n",
    "\\end{align}$\n",
    "\n",
    "Note that $A^T A + \\lambda I$ always has an inverse for $\\lambda \\in (0,1]$ since the sum of a positive semidefinite and positive definite matrix is still positive definite, and so we have $\\mathbf{x} = (A^T A + \\lambda I)^{-1} A^T \\mathbf{b}$. In this case $\\nabla^2 L(x) = 2 A^T A + 2 \\lambda I$. This is independent of $\\mathbf{x}$. It is also positive definite, so any stationary point of $L(\\mathbf{x})$ is a minimum point. Therefore the analytical solution to the regularised least squares problem is $\\mathbf{x} = (A^T A + \\lambda I)^{-1} A^T \\mathbf{b}$ for $\\lambda \\in (0,1]$.\n",
    "\n",
    "Now if $\\lambda = 0$ the matrix $A^T A$ also has an inverse if it is of full rank, so the above expression will still hold and we have $\\mathbf{x} = (A^T A )^{-1} A^T \\mathbf{b}$ . Otherwise, the problem is underdetermined and we cannot find an analytical expression for the exact solution $\\mathbf{x}$ (although the least squares problem can still be solved)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOIp8Mk9r340"
   },
   "source": [
    "***\n",
    "### Question 6 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNvM4MeYvkIZ",
    "outputId": "d97941ef-46ec-4132-f063-a3aec3cc1c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(410)\n",
      "tensor(0.)\n",
      "tensor(1.5519e+08)\n"
     ]
    }
   ],
   "source": [
    "# Read the csv files into pandas dataframes\n",
    "A_df = pd.read_csv('A.csv',header=None)\n",
    "b_df = pd.read_csv('b.csv',header=None)\n",
    "\n",
    "# print(b_df.shape)\n",
    "\n",
    "# Convert the pandas dataframes into pytorch tensors of float32s\n",
    "A = torch.tensor(A_df.values, dtype=torch.float32)\n",
    "b = torch.tensor(b_df.values, dtype=torch.float32)\n",
    "\n",
    "# print(A.shape)\n",
    "print(torch.linalg.matrix_rank(A.T @ A))\n",
    "print(torch.linalg.det(A.T @ A))\n",
    "print(torch.linalg.cond(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUOPDRU3vqEE"
   },
   "source": [
    "Here we can see that the matrix $A$ (and later $A_\\text{train}$) have a high condition number. As such when solving the problem $A^T A \\mathbf{x} = A^T \\mathbf{b}$ we cannot simply solve the equation $A \\mathbf{x} = \\mathbf{b}$ with torch.linalg.solve: the system is extremely senstive to perturbations and we might run into numerical instability issues. Therefore we make use of the least squares minimisation problem in PyTorch rather than directly solving this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceW7w_-Or340",
    "outputId": "8ea77d67-9965-4913-975a-c8ac75d25a71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS estimator - Testing Error:  2.624615247359685e-10\n",
      "tensor(1392796.8750)\n"
     ]
    }
   ],
   "source": [
    "# Redefine the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Split the dataset in a similar way as in q4\n",
    "perm = torch.randperm(A.shape[0])\n",
    "\n",
    "train_size = int(0.8 * A.shape[0])\n",
    "train_indx = perm[:train_size]\n",
    "test_indx = perm[train_size:]\n",
    "\n",
    "A_train = A[train_idx]\n",
    "b_train = b[train_idx]\n",
    "A_test = A[test_idx]\n",
    "b_test = b[test_idx]\n",
    "\n",
    "# We aim to solve the equation in q5 x= (A^TA + lambda*I)^-1A^Tb\n",
    "# When lambda = 0 this reduces to A^TAx = A^Tb\n",
    "# We make use of lstsq in torch.linalg to leverage numerically stable methods to solve this least squares problem\n",
    "# This is because the matrix we are using is ill-conditioned\n",
    "x_ols = torch.linalg.lstsq(A_train, b_train,driver='gelsd').solution\n",
    "\n",
    "# Find the error of the OLS estimator on the testing data\n",
    "# ols_test_error = compute_mse(A_test, b_test, x_ols)\n",
    "\n",
    "b_pred_test = A_test @ x_ols\n",
    "ols_test_error = torch.mean((b_test - b_pred_test)**2)\n",
    "\n",
    "print(\"OLS estimator - Testing Error: \", ols_test_error.item())\n",
    "\n",
    "print(torch.linalg.cond(A_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykbP5KhMr340"
   },
   "source": [
    "***\n",
    "### Question 7 [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6xQIFCCr340",
    "outputId": "b66ee805-ed5d-47fe-aaa1-a7431f1df8fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent with Autograd - Testing Error:  2.3508493995905155e-06\n",
      "OLS estimator - Testing Error:  2.624615247359685e-10\n"
     ]
    }
   ],
   "source": [
    "# Redefine the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def gradient_descent_autograd(X, y, lambda_, learning_rate=0.1, epochs=1000):\n",
    "\n",
    "    # Initialise all parameters as 0 (with the same dtype as entries of X)\n",
    "    x_gd = torch.zeros((A_train.shape[1], 1), requires_grad=True)\n",
    "\n",
    "    # Iterate the gradient descent algorithm through the given number of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Compute the loss L(x) = ||Ax - b||^2 since lambda = 0\n",
    "        # loss = torch.norm(A_train @ x_gd - b_train, p=2) ** 2\n",
    "        loss = torch.sum((A_train @ x_gd - b_train) ** 2) + lambda_ * torch.sum(x_gd ** 2)\n",
    "\n",
    "        # Compute gradients using autograd\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters in the direction of steepest descent\n",
    "        # Do not calculate gradients for parameter updates (we only do this for the loss)\n",
    "        with torch.no_grad():\n",
    "            x_gd -= learning_rate * x_gd.grad\n",
    "            x_gd.grad.zero_()\n",
    "\n",
    "    # Return the final parameters x_gd\n",
    "    # detach in PyTorch stops gradients from being tracked - we no longer require gradient calculations\n",
    "    return x_gd.detach()\n",
    "\n",
    "# Compute test error\n",
    "x_gd_autograd = gradient_descent_autograd(A_train, b_train, 0)\n",
    "gd_autograd_test_error = compute_mse(A_test, b_test, x_gd_autograd)\n",
    "print(\"Gradient Descent with Autograd - Testing Error: \", gd_autograd_test_error.item())\n",
    "print(\"OLS estimator - Testing Error: \", ols_test_error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfbYzUF_qe0p"
   },
   "source": [
    "The testing error using gradient descent with autograd is a factor of $10^4$ greater than the testing error using the analytic OLS solution. The loss is convex again so gradient descent is guaranteed to converge; however the number of epochs may be too low or the learning rate may be too low, causing the gradient descent algorithm to not converge in time.\n",
    "\n",
    "Since the condition number of both $A$ and $A_\\text{train}$ are extremely high ($1.55 \\times 10^8$ and $1.39 \\times 10^6$ respectively) we make use of the gelsd driver in torch.linalg.lstsq to use numerically stable methods, including tridiagonal reduction and SVD. The high condition number causes the gradient descent testing error to be higher - in order to improve the error one could consider introducing momentum to improve the speed of convergence (due to the square root term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqAALRLKr340"
   },
   "source": [
    "***\n",
    "### Question 8 [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "xBqZnC-iOHsu",
    "outputId": "8ac24309-ed46-4384-e3dc-cc806744ea30"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Redefine the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "lambdas = np.logspace(-8,0)\n",
    "testing_errors = np.zeros(lambdas.size)\n",
    "\n",
    "# Iterate through each lambda value\n",
    "for i, lamb in enumerate(lambdas):\n",
    "  # Find the testing error (MSE) for each lambda\n",
    "  x_gd_autograd = gradient_descent_autograd(A, b, lamb)\n",
    "  testing_errors[i] = compute_mse(A_test, b_test, x_gd_autograd)\n",
    "\n",
    "#Plot the graph of error against lambda\n",
    "plt.plot(lambdas, testing_errors)\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel('Testing Error')\n",
    "plt.title(r'Testing Error vs. $\\lambda$')\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuOP42u3mI16"
   },
   "source": [
    "We need to choose a small enough value of $\\lambda$ such that the testing error is sufficiently small. However, we do not want $\\lambda$ to be too small otherwise the penalty introduced by the regularisation is too small. The purpose of the penalty term is to prevent the coefficients being too large, leading to overfitting and poor generalisation. As such a value of lambda of $\\approx 10^{-2}$ ensures the testing error is close to the minimum value (if $\\lambda$ is greater than the error starts to grows rapidly); reducing $\\lambda$ further does not significantly decrease the error. This is an exmaple of bias-variance trade-off: by introducing the penalty/regularisation term we sacrifice the unbiased nature of the estimator but reduce the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DJzd6r3FoGLr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
